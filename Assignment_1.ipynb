{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd4ed572",
   "metadata": {},
   "source": [
    "# CSV VS PARQUET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71995013",
   "metadata": {},
   "source": [
    "The benchmarks and results from this notebook have been saved and used in an interactive Streamlit dashboard. You can directly run the Streamlit dashboard to explore the results visually.\n",
    "\n",
    "To launch the dashboard, run the following command in the terminal:\n",
    "\n",
    "\n",
    "streamlit run streamlit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "438b9543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import polars as pl\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21651c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in c:\\users\\nandi\\anaconda3\\lib\\site-packages (1.23.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b7b6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path =\"all_stocks_5yr.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efdc9fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "270b94dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV File Size: 28.704510 MB\n"
     ]
    }
   ],
   "source": [
    "csv_size =os.path.getsize(csv_path)/(1024 *1024)\n",
    "print(f\"CSV File Size: {csv_size:2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7ca900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-02-08</td>\n",
       "      <td>15.07</td>\n",
       "      <td>15.12</td>\n",
       "      <td>14.63</td>\n",
       "      <td>14.75</td>\n",
       "      <td>8407500</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-02-11</td>\n",
       "      <td>14.89</td>\n",
       "      <td>15.01</td>\n",
       "      <td>14.26</td>\n",
       "      <td>14.46</td>\n",
       "      <td>8882000</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-02-12</td>\n",
       "      <td>14.45</td>\n",
       "      <td>14.51</td>\n",
       "      <td>14.10</td>\n",
       "      <td>14.27</td>\n",
       "      <td>8126000</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-02-13</td>\n",
       "      <td>14.30</td>\n",
       "      <td>14.94</td>\n",
       "      <td>14.25</td>\n",
       "      <td>14.66</td>\n",
       "      <td>10259500</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-02-14</td>\n",
       "      <td>14.94</td>\n",
       "      <td>14.96</td>\n",
       "      <td>13.16</td>\n",
       "      <td>13.99</td>\n",
       "      <td>31879900</td>\n",
       "      <td>AAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   open   high    low  close    volume name\n",
       "0  2013-02-08  15.07  15.12  14.63  14.75   8407500  AAL\n",
       "1  2013-02-11  14.89  15.01  14.26  14.46   8882000  AAL\n",
       "2  2013-02-12  14.45  14.51  14.10  14.27   8126000  AAL\n",
       "3  2013-02-13  14.30  14.94  14.25  14.66  10259500  AAL\n",
       "4  2013-02-14  14.94  14.96  13.16  13.99  31879900  AAL"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca9f380f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 619040 entries, 0 to 619039\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   date    619040 non-null  object \n",
      " 1   open    619029 non-null  float64\n",
      " 2   high    619032 non-null  float64\n",
      " 3   low     619032 non-null  float64\n",
      " 4   close   619040 non-null  float64\n",
      " 5   volume  619040 non-null  int64  \n",
      " 6   name    619040 non-null  object \n",
      "dtypes: float64(4), int64(1), object(2)\n",
      "memory usage: 33.1+ MB\n",
      "None\n",
      "                open           high            low          close  \\\n",
      "count  619029.000000  619032.000000  619032.000000  619040.000000   \n",
      "mean       83.023334      83.778311      82.256096      83.043763   \n",
      "std        97.378769      98.207519      96.507421      97.389748   \n",
      "min         1.620000       1.690000       1.500000       1.590000   \n",
      "25%        40.220000      40.620000      39.830000      40.245000   \n",
      "50%        62.590000      63.150000      62.020000      62.620000   \n",
      "75%        94.370000      95.180000      93.540000      94.410000   \n",
      "max      2044.000000    2067.990000    2035.110000    2049.000000   \n",
      "\n",
      "             volume  \n",
      "count  6.190400e+05  \n",
      "mean   4.321823e+06  \n",
      "std    8.693610e+06  \n",
      "min    0.000000e+00  \n",
      "25%    1.070320e+06  \n",
      "50%    2.082094e+06  \n",
      "75%    4.284509e+06  \n",
      "max    6.182376e+08  \n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9902f34",
   "metadata": {},
   "source": [
    "# PARQUET COMPRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d05a72c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"all_stocks_5yr.parquet\"\n",
    "start_time = time.time()\n",
    "df.to_parquet(parquet_path, engine='pyarrow', compression='snappy', index=False)\n",
    "parquet_write_time = time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7e02cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV File Size: 28.70 MB\n",
      "Parquet File Size: 10.15 MB\n"
     ]
    }
   ],
   "source": [
    "csv_size = os.path.getsize(csv_path) / (1024 * 1024)  # Convert to MB\n",
    "parquet_size = os.path.getsize(parquet_path) / (1024 * 1024)\n",
    "print(f\"CSV File Size: {csv_size:.2f} MB\")\n",
    "print(f\"Parquet File Size: {parquet_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70a231bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df.to_csv(\"temp_1x.csv\", index=False)\n",
    "csv_write_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be2e0372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.570556163787842\n"
     ]
    }
   ],
   "source": [
    "print(csv_write_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "757d4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pd.read_csv(\"temp_1x.csv\")\n",
    "csv_read_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a266768d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38915443420410156\n"
     ]
    }
   ],
   "source": [
    "print(csv_read_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6227e847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet Write Time (1x): 0.2741 sec\n"
     ]
    }
   ],
   "source": [
    "# Measure Parquet Write Time for 1x\n",
    "start_time = time.time()\n",
    "table = pa.Table.from_pandas(df)  # Convert DataFrame to Parquet Table\n",
    "pq.write_table(table, \"all_stocks_5yr_snappy.parquet\", compression=\"snappy\")\n",
    "parquet_write_time = time.time() - start_time\n",
    "\n",
    "# Print Parquet Write Time\n",
    "print(f\"Parquet Write Time (1x): {parquet_write_time:.4f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "147e2f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "pd.read_parquet(parquet_path)\n",
    "parquet_read_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff1eb20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14321351051330566\n"
     ]
    }
   ],
   "source": [
    "print(parquet_read_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb8d0c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset scaled to 10x. New shape: (6190400, 7)\n"
     ]
    }
   ],
   "source": [
    "df_10x = pd.concat([df] * 10, ignore_index=True)\n",
    "print(f\"Dataset scaled to 10x. New shape: {df_10x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81296368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 10x Storage Size Comparison:\n",
      "CSV File Size: 288.01 MB\n",
      "Parquet File Size: 87.58 MB\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "csv_10x_path = \"all_stocks_5yr_10x.csv\"\n",
    "parquet_10x_path = \"all_stocks_5yr_10x.parquet\"\n",
    "\n",
    "\n",
    "# Get File Sizes\n",
    "csv_size_10x = os.path.getsize(csv_10x_path) / (1024 * 1024)  # Convert to MB\n",
    "parquet_size_10x = os.path.getsize(parquet_10x_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"\\n 10x Storage Size Comparison:\")\n",
    "print(f\"CSV File Size: {csv_size_10x:.2f} MB\")\n",
    "print(f\"Parquet File Size: {parquet_size_10x:.2f} MB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "add0f7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 10x Read Time Comparison:\n",
      "CSV Read Time: 3.4856 sec\n",
      "Parquet Read Time: 1.0929 sec\n"
     ]
    }
   ],
   "source": [
    "# Measure CSV Read Time\n",
    "start_time = time.time()\n",
    "pd.read_csv(csv_10x_path)\n",
    "csv_read_time_10x = time.time() - start_time\n",
    "\n",
    "# Measure Parquet Read Time\n",
    "start_time = time.time()\n",
    "pq.read_table(parquet_10x_path).to_pandas()\n",
    "parquet_read_time_10x = time.time() - start_time\n",
    "\n",
    "print(f\"\\n 10x Read Time Comparison:\")\n",
    "print(f\"CSV Read Time: {csv_read_time_10x:.4f} sec\")\n",
    "print(f\"Parquet Read Time: {parquet_read_time_10x:.4f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5203ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 10x Write Time Comparison:\n",
      "CSV Write Time: 25.1147 sec\n",
      "Parquet Write Time: 2.1718 sec\n"
     ]
    }
   ],
   "source": [
    "# Measure CSV Write Time\n",
    "start_time = time.time()\n",
    "df_10x.to_csv(csv_10x_path, index=False)\n",
    "csv_write_time_10x = time.time() - start_time\n",
    "\n",
    "# Measure Parquet Write Time\n",
    "start_time = time.time()\n",
    "table_10x = pa.Table.from_pandas(df_10x)\n",
    "pq.write_table(table_10x, parquet_10x_path, compression=\"snappy\")\n",
    "parquet_write_time_10x = time.time() - start_time\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n 10x Write Time Comparison:\")\n",
    "print(f\"CSV Write Time: {csv_write_time_10x:.4f} sec\")\n",
    "print(f\"Parquet Write Time: {parquet_write_time_10x:.4f} sec\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37521dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset scaled to 100x. New shape: (6190400, 7)\n"
     ]
    }
   ],
   "source": [
    "df_100x = pd.concat([df] * 100, ignore_index=True)\n",
    "print(f\"Dataset scaled to 100x. New shape: {df_10x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b32f7264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Optimized CSV Write Time (100x): 284.1913 sec\n",
      "\n",
      "✅ 100x Write Time Comparison:\n",
      "CSV Write Time: 284.1913 sec\n",
      "Parquet Write Time: 25.9207 sec\n",
      "\n",
      "✅ 100x Storage Size Comparison:\n",
      "CSV File Size: 2880.05 MB\n",
      "Parquet File Size: 861.61 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define file paths\n",
    "csv_100x_path = \"all_stocks_5yr_100x.csv\"\n",
    "parquet_100x_path = \"all_stocks_5yr_100x.parquet\"\n",
    "\n",
    "#  Efficient CSV Writing using Chunks\n",
    "chunk_size = 500_000  # Adjust as needed\n",
    "start_time = time.time()\n",
    "\n",
    "with open(csv_100x_path, 'w', newline='') as file:\n",
    "    for i in range(0, len(df_100x), chunk_size):\n",
    "        df_100x.iloc[i:i+chunk_size].to_csv(file, index=False, header=(i == 0))  # Write header only for first chunk\n",
    "\n",
    "csv_write_time_100x = time.time() - start_time\n",
    "print(f\" Optimized CSV Write Time (100x): {csv_write_time_100x:.4f} sec\")\n",
    "\n",
    "# Efficient Parquet Writing with Compression\n",
    "start_time = time.time()\n",
    "table_100x = pa.Table.from_pandas(df_100x)\n",
    "pq.write_table(table_100x, parquet_100x_path, compression=\"snappy\")\n",
    "parquet_write_time_100x = time.time() - start_time\n",
    "\n",
    "# Get File Sizes\n",
    "csv_size_100x = os.path.getsize(csv_100x_path) / (1024 * 1024)  # Convert to MB\n",
    "parquet_size_100x = os.path.getsize(parquet_100x_path) / (1024 * 1024)\n",
    "\n",
    "#  Print Results\n",
    "print(f\"\\n✅ 100x Write Time Comparison:\")\n",
    "print(f\"CSV Write Time: {csv_write_time_100x:.4f} sec\")\n",
    "print(f\"Parquet Write Time: {parquet_write_time_100x:.4f} sec\")\n",
    "\n",
    "print(f\"\\n✅ 100x Storage Size Comparison:\")\n",
    "print(f\"CSV File Size: {csv_size_100x:.2f} MB\")\n",
    "print(f\"Parquet File Size: {parquet_size_100x:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bff6a904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 100x Read Time Comparison:\n",
      "CSV Read Time: 54.9933 sec\n",
      "Parquet Read Time: 17.8047 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    " # Measure Read Time for 100x CSV\n",
    "start_time = time.time()\n",
    "df_csv_100x = pd.read_csv(csv_100x_path)\n",
    "csv_read_time_100x = time.time() - start_time\n",
    "\n",
    "# Measure Read Time for 100x Parquet\n",
    "start_time = time.time()\n",
    "df_parquet_100x = pq.read_table(parquet_100x_path).to_pandas()\n",
    "parquet_read_time_100x = time.time() - start_time\n",
    "\n",
    "#  Print Results\n",
    "print(f\"\\n 100x Read Time Comparison:\")\n",
    "print(f\"CSV Read Time: {csv_read_time_100x:.4f} sec\")\n",
    "print(f\"Parquet Read Time: {parquet_read_time_100x:.4f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73afade1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Data Loading Time Comparison:\n",
      "Pandas Load Time: 0.1771 sec\n",
      "Polars Load Time: 0.1848 sec\n"
     ]
    }
   ],
   "source": [
    "parquet_path = \"all_stocks_5yr.parquet\"\n",
    "\n",
    "# Load Data with Pandas\n",
    "start_time = time.time()\n",
    "df_pandas = pd.read_parquet(parquet_path)\n",
    "pandas_load_time = time.time() - start_time\n",
    "\n",
    "#  Load Data with Polars\n",
    "start_time = time.time()\n",
    "df_polars = pl.read_parquet(parquet_path)\n",
    "polars_load_time = time.time() - start_time\n",
    "\n",
    "#  Print Results\n",
    "print(f\"\\n Data Loading Time Comparison:\")\n",
    "print(f\"Pandas Load Time: {pandas_load_time:.4f} sec\")\n",
    "print(f\"Polars Load Time: {polars_load_time:.4f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bde736",
   "metadata": {},
   "source": [
    "# Calculate Moving Average "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3544954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Data with Indicators (Pandas):\n",
      "         date   open   high    low  close    volume name  SMA_20     EMA_20  \\\n",
      "0  2013-02-08  15.07  15.12  14.63  14.75   8407500  AAL     NaN  14.750000   \n",
      "1  2013-02-11  14.89  15.01  14.26  14.46   8882000  AAL     NaN  14.722381   \n",
      "2  2013-02-12  14.45  14.51  14.10  14.27   8126000  AAL     NaN  14.679297   \n",
      "3  2013-02-13  14.30  14.94  14.25  14.66  10259500  AAL     NaN  14.677459   \n",
      "4  2013-02-14  14.94  14.96  13.16  13.99  31879900  AAL     NaN  14.611987   \n",
      "\n",
      "   RSI_14      MACD  \n",
      "0     NaN  0.000000  \n",
      "1     NaN -0.023134  \n",
      "2     NaN -0.056152  \n",
      "3     NaN -0.050270  \n",
      "4     NaN -0.098535  \n",
      "\n",
      "Sample Data with Indicators (Polars):\n",
      "shape: (5, 11)\n",
      "┌────────────┬───────┬───────┬───────┬───┬────────┬───────────┬────────┬───────────┐\n",
      "│ date       ┆ open  ┆ high  ┆ low   ┆ … ┆ SMA_20 ┆ EMA_20    ┆ RSI_14 ┆ MACD      │\n",
      "│ ---        ┆ ---   ┆ ---   ┆ ---   ┆   ┆ ---    ┆ ---       ┆ ---    ┆ ---       │\n",
      "│ str        ┆ f64   ┆ f64   ┆ f64   ┆   ┆ f64    ┆ f64       ┆ f64    ┆ f64       │\n",
      "╞════════════╪═══════╪═══════╪═══════╪═══╪════════╪═══════════╪════════╪═══════════╡\n",
      "│ 2013-02-08 ┆ 15.07 ┆ 15.12 ┆ 14.63 ┆ … ┆ null   ┆ 14.75     ┆ null   ┆ 0.0       │\n",
      "│ 2013-02-11 ┆ 14.89 ┆ 15.01 ┆ 14.26 ┆ … ┆ null   ┆ 14.59775  ┆ null   ┆ -0.006506 │\n",
      "│ 2013-02-12 ┆ 14.45 ┆ 14.51 ┆ 14.1  ┆ … ┆ null   ┆ 14.477402 ┆ null   ┆ -0.014182 │\n",
      "│ 2013-02-13 ┆ 14.3  ┆ 14.94 ┆ 14.25 ┆ … ┆ null   ┆ 14.530115 ┆ null   ┆ -0.003245 │\n",
      "│ 2013-02-14 ┆ 14.94 ┆ 14.96 ┆ 13.16 ┆ … ┆ null   ┆ 14.399466 ┆ null   ┆ -0.023895 │\n",
      "└────────────┴───────┴───────┴───────┴───┴────────┴───────────┴────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load dataset\n",
    "df_pandas = pd.read_parquet(\"all_stocks_5yr.parquet\")\n",
    "df_polars = pl.read_parquet(\"all_stocks_5yr.parquet\")\n",
    "\n",
    "# Simple Moving Average (SMA)\n",
    "df_pandas[\"SMA_20\"] = df_pandas[\"close\"].rolling(20).mean()\n",
    "df_polars = df_polars.with_columns(pl.col(\"close\").rolling_mean(20).alias(\"SMA_20\"))\n",
    "\n",
    "# Exponential Moving Average (EMA)\n",
    "df_pandas[\"EMA_20\"] = df_pandas[\"close\"].ewm(span=20, adjust=False).mean()\n",
    "df_polars = df_polars.with_columns(pl.col(\"close\").ewm_mean(com=9.5).alias(\"EMA_20\"))  # Fixed\n",
    "\n",
    "# Relative Strength Index (RSI) Function\n",
    "def calculate_rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(window).mean()\n",
    "    loss = -delta.where(delta < 0, 0).rolling(window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "df_pandas[\"RSI_14\"] = calculate_rsi(df_pandas[\"close\"])\n",
    "df_polars = df_polars.with_columns(\n",
    "    (100 - (100 / (1 +\n",
    "        pl.col(\"close\").diff().clip(0, None).rolling_mean(14) /\n",
    "        pl.col(\"close\").diff().clip(None, 0).abs().rolling_mean(14)\n",
    "    ))).alias(\"RSI_14\")\n",
    ")\n",
    "\n",
    "# Moving Average Convergence Divergence (MACD)\n",
    "df_pandas[\"MACD\"] = df_pandas[\"close\"].ewm(span=12, adjust=False).mean() - df_pandas[\"close\"].ewm(span=26, adjust=False).mean()\n",
    "df_polars = df_polars.with_columns(\n",
    "    (pl.col(\"close\").ewm_mean(com=5.5) - pl.col(\"close\").ewm_mean(com=12.5)).alias(\"MACD\")\n",
    ")\n",
    "\n",
    "# Display Sample Data\n",
    "print(\"\\nSample Data with Indicators (Pandas):\")\n",
    "print(df_pandas.head())\n",
    "\n",
    "print(\"\\nSample Data with Indicators (Polars):\")\n",
    "print(df_polars.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a81f5b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pandas vs Polars Execution Time:\n",
      "Pandas Execution Time: 0.1426 sec\n",
      "Polars Execution Time: 0.0652 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "df_pandas[\"SMA_20\"] = df_pandas[\"close\"].rolling(window=20).mean()\n",
    "df_pandas[\"EMA_20\"] = df_pandas[\"close\"].ewm(span=20, adjust=False).mean()\n",
    "df_pandas[\"RSI_14\"] = calculate_rsi(df_pandas[\"close\"])\n",
    "df_pandas[\"MACD\"] = df_pandas[\"close\"].ewm(span=12, adjust=False).mean() - df_pandas[\"close\"].ewm(span=26, adjust=False).mean()\n",
    "pandas_time = time.time() - start_time\n",
    "\n",
    "# Measure Polars Execution Time\n",
    "start_time = time.time()\n",
    "df_polars = df_polars.with_columns([\n",
    "    pl.col(\"close\").rolling_mean(20).alias(\"SMA_20\"),\n",
    "    pl.col(\"close\").ewm_mean(com=9.5).alias(\"EMA_20\"),  # Converted from span=20\n",
    "    (100 - (100 / (1 +\n",
    "        pl.col(\"close\").diff().clip(0, None).rolling_mean(14) /\n",
    "        pl.col(\"close\").diff().clip(None, 0).abs().rolling_mean(14)\n",
    "    ))).alias(\"RSI_14\"),\n",
    "    (pl.col(\"close\").ewm_mean(com=5.5) - pl.col(\"close\").ewm_mean(com=12.5)).alias(\"MACD\")  # Converted from span\n",
    "])\n",
    "polars_time = time.time() - start_time\n",
    "\n",
    "# Print Performance Results\n",
    "print(f\"\\nPandas vs Polars Execution Time:\")\n",
    "print(f\"Pandas Execution Time: {pandas_time:.4f} sec\")\n",
    "print(f\"Polars Execution Time: {polars_time:.4f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "844392a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define features and target\n",
    "features = [\"SMA_20\", \"EMA_20\", \"RSI_14\", \"MACD\"]\n",
    "target = \"close\"\n",
    "\n",
    "# Drop NaN values (since indicators have missing values at the start)\n",
    "df_pandas = df_pandas.dropna()\n",
    "\n",
    "# Extract feature matrix (X) and target variable (y)\n",
    "X = df_pandas[features]\n",
    "y = df_pandas[target]\n",
    "\n",
    "# Split data into train (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features for better performance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0caead90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear Regression Model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "lr_preds = lr_model.predict(X_test_scaled)\n",
    "lr_rmse = mean_squared_error(y_test, lr_preds, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "023ee31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Lasso Regression Performance:\n",
      "RMSE: 6.3320\n",
      "Training Time: 1.7965 sec\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Train Lasso Regression Model\n",
    "lasso_model = Lasso(alpha=0.1)  # `alpha` controls feature selection\n",
    "\n",
    "start_time = time.time()\n",
    "lasso_model.fit(X_train_scaled, y_train)  # Train model\n",
    "lasso_time = time.time() - start_time\n",
    "\n",
    "# Predict and calculate RMSE\n",
    "lasso_preds = lasso_model.predict(X_test_scaled)\n",
    "lasso_rmse = mean_squared_error(y_test, lasso_preds, squared=False)\n",
    "\n",
    "# Print Results\n",
    "print(\"\\n✅ Lasso Regression Performance:\")\n",
    "print(f\"RMSE: {lasso_rmse:.4f}\")\n",
    "print(f\"Training Time: {lasso_time:.4f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c573cda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Gradient Boosting Performance:\n",
      "RMSE: 3.7749\n",
      "Training Time: 189.6535 sec\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Train Gradient Boosting Model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=50, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "\n",
    "start_time = time.time()\n",
    "gb_model.fit(X_train_scaled, y_train)  # Train model\n",
    "gb_time = time.time() - start_time\n",
    "\n",
    "# Predict and calculate RMSE\n",
    "gb_preds = gb_model.predict(X_test_scaled)\n",
    "gb_rmse = mean_squared_error(y_test, gb_preds, squared=False)\n",
    "\n",
    "# Print Results\n",
    "print(\"\\n✅ Gradient Boosting Performance:\")\n",
    "print(f\"RMSE: {gb_rmse:.4f}\")\n",
    "print(f\"Training Time: {gb_time:.4f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3cca9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008774 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 495208, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 83.017058\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      " LightGBM Performance:\n",
      "RMSE: 19.7446\n",
      "Training Time: 0.5360 sec\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "# Train LightGBM Model\n",
    "lgb_model = LGBMRegressor(n_estimators=50, max_depth=5, learning_rate=0.1, n_jobs=-1, random_state=42)\n",
    "\n",
    "start_time = time.time()\n",
    "lgb_model.fit(X_train_scaled, y_train)  # Train LightGBM\n",
    "lgb_time = time.time() - start_time\n",
    "\n",
    "# Predict and calculate RMSE\n",
    "lgb_preds = lgb_model.predict(X_test_scaled)\n",
    "lgb_rmse = mean_squared_error(y_test, lgb_preds, squared=False)\n",
    "\n",
    "# Print Results\n",
    "print(\"\\n LightGBM Performance:\")\n",
    "print(f\"RMSE: {lgb_rmse:.4f}\")\n",
    "print(f\"Training Time: {lgb_time:.4f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7148b976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\nandi\\anaconda3\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from lightgbm) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from lightgbm) (1.11.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c14fdba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in c:\\users\\nandi\\anaconda3\\lib\\site-packages (1.42.2)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (8.0.4)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (1.24.3)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (23.1)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (2.0.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (9.4.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (5.29.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (11.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (2.31.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (2.1.6)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from streamlit) (6.3.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.17.3)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (1.28.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2023.7.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8d947f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Benchmark results saved to 'benchmark_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with benchmarking results using the correct variables\n",
    "benchmark_results = pd.DataFrame({\n",
    "    \"Metric\": [\n",
    "        \"CSV Read Time (1x)\", \"Parquet Read Time (1x)\",\n",
    "        \"CSV Read Time (10x)\", \"Parquet Read Time (10x)\",\n",
    "        \"CSV Read Time (100x)\", \"Parquet Read Time (100x)\",\n",
    "        \"CSV Write Time (1x)\", \"Parquet Write Time (1x)\",\n",
    "        \"CSV Write Time (10x)\", \"Parquet Write Time (10x)\",\n",
    "        \"CSV Write Time (100x)\", \"Parquet Write Time (100x)\",\n",
    "        \"Pandas Execution Time\", \"Polars Execution Time\",\n",
    "        \"Pandas Load Time\", \"Polars Load Time\",\n",
    "        \"Lasso Train Time\", \"GB Train Time\", \"LGB Train Time\",\n",
    "        \"Lasso MSE\", \"GB MSE\", \"LGB MSE\"\n",
    "    ],\n",
    "    \"Value (Seconds/Error)\": [\n",
    "        csv_read_time, parquet_read_time,\n",
    "        csv_read_time_10x, parquet_read_time_10x,\n",
    "        csv_read_time_100x, parquet_read_time_100x,\n",
    "        csv_write_time, parquet_write_time,\n",
    "        csv_write_time_10x, parquet_write_time_10x,\n",
    "        csv_write_time_100x, parquet_write_time_100x,\n",
    "        pandas_time, polars_time,\n",
    "        pandas_load_time, polars_load_time,\n",
    "        lasso_time, gb_time, lgb_time,\n",
    "        lasso_rmse, gb_rmse, lgb_rmse\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "benchmark_results.to_csv(\"benchmark_results.csv\", index=False)\n",
    "print(\"✅ Benchmark results saved to 'benchmark_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcde9c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trained model saved as 'trained_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(gb_model, \"trained_model.pkl\")\n",
    "\n",
    "print(\"✅ Trained model saved as 'trained_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "461b6d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', 'app.py']>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Run Streamlit in the background without blocking Jupyter Notebook\n",
    "subprocess.Popen([\"streamlit\", \"run\", \"app.py\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a80e677f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', 'app.py']>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.Popen([\"streamlit\", \"run\", \"app.py\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d971ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\nandi\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nandi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5a2ffeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model expects 4 features.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load(\"trained_model.pkl\")\n",
    "\n",
    "# Print the expected number of input features\n",
    "print(f\"Model expects {model.n_features_in_} features.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f113a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Features: ['date', 'open', 'high', 'low', 'volume', 'name']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset used for training\n",
    "df = pd.read_csv(\"all_stocks_5yr.csv\")  # Replace with your actual dataset file\n",
    "\n",
    "# Get the feature columns (excluding the target column)\n",
    "feature_columns = df.drop(columns=[\"close\"]).columns  # Assuming \"close\" is the target variable\n",
    "\n",
    "print(\"Model Features:\", list(feature_columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f88aa6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scaler saved as 'scaler.pkl'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming X_train was used for training\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Save the trained scaler\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "print(\"✅ Scaler saved as 'scaler.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20388f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
